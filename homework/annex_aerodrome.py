import os
import json
import re
import hashlib
from typing import List, Dict, Tuple, Optional, Any
from collections import defaultdict
from dataclasses import dataclass
import warnings
import sys
import time

warnings.filterwarnings('ignore')

# è®¾ç½®ç¡…åŸºæµåŠ¨API
import openai
from openai import OpenAI

# é…ç½®API
client = OpenAI(
    api_key="sk-bdgrimfksplnwstzulxfsrdijhjqribunforxvknatzpjlui",
    base_url="https://api.siliconflow.cn/v1"
)


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    content: str
    chapter: str
    section: str
    confidence: float
    source_page: Optional[str] = None
    keywords: List[str] = None


@dataclass
class ConversationTurn:
    """å¯¹è¯è½®æ¬¡æ•°æ®ç»“æ„"""
    question: str
    answer: str
    references: List[Dict]
    timestamp: float


class EnhancedAttachment14ManualQA:
    def __init__(self, manual_path: str, max_context_length: int = 32000):
        """
        å¢å¼ºç‰ˆé™„ä»¶14æ‰‹å†Œé—®ç­”ç³»ç»Ÿ

        Args:
            manual_path: æ‰‹å†Œæ–‡ä»¶è·¯å¾„
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆtokensï¼‰
        """
        self.manual_path = manual_path
        self.max_context_length = max_context_length
        self.content = self._load_manual()
        self.structure = self._parse_structure()
        self.chunked_content = self._chunk_content()
        self.conversation_history: List[ConversationTurn] = []
        self.keyword_index = self._build_keyword_index()
        self.session_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]

        print(f"âœ“ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ åŠ è½½ç« èŠ‚: {len(self.structure['chapters'])}ä¸ª")
        print(f"âœ“ å†…å®¹å—æ•°: {len(self.chunked_content)}ä¸ª")
        print(f"âœ“ ç´¢å¼•å…³é”®è¯: {len(self.keyword_index)}ä¸ª")

    def _load_manual(self) -> str:
        """åŠ è½½æ‰‹å†Œå†…å®¹"""
        try:
            with open(self.manual_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                print(f"âœ“ å·²åŠ è½½æ‰‹å†Œï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                return content
        except Exception as e:
            print(f"âœ— åŠ è½½æ‰‹å†Œå¤±è´¥: {e}")
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(self.manual_path, 'r', encoding='gbk', errors='ignore') as f:
                    content = f.read()
                    print(f"âœ“ å·²åŠ è½½æ‰‹å†Œ(GBKç¼–ç )ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                    return content
            except:
                return ""

    def _parse_structure(self) -> Dict:
        """è§£ææ‰‹å†Œç»“æ„"""
        structure = {
            "chapters": {},
            "sections": {},
            "definitions": {},
            "tables": {},
            "figures": {},
            "toc": []  # ç›®å½•æ¡ç›®
        }

        # æå–æ‰€æœ‰æ ‡é¢˜ç»“æ„
        lines = self.content.split('\n')
        current_chapter = None

        for line in lines:
            line = line.strip()

            # æå–ç« èŠ‚
            chapter_match = re.match(r'^##\s*ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)\s*ç« \s*(.+)$', line)
            if chapter_match:
                chapter_num = chapter_match.group(1)
                chapter_title = chapter_match.group(2).strip()
                chapter_key = f"ç¬¬{chapter_num}ç« "
                structure["chapters"][chapter_key] = chapter_title
                structure["toc"].append({
                    "level": 2,
                    "type": "chapter",
                    "number": chapter_key,
                    "title": chapter_title
                })
                current_chapter = chapter_key
                continue

            # æå–å°èŠ‚
            section_match = re.match(r'^###\s*(\d+\.\d+(?:\.\d+)*)\s*(.+)$', line)
            if section_match and current_chapter:
                section_num = section_match.group(1)
                section_title = section_match.group(2).strip()
                section_key = f"{current_chapter}_{section_num}"
                structure["sections"][section_key] = {
                    "title": section_title,
                    "chapter": current_chapter,
                    "number": section_num
                }
                structure["toc"].append({
                    "level": 3,
                    "type": "section",
                    "chapter": current_chapter,
                    "number": section_num,
                    "title": section_title
                })
                continue

            # æå–å­å°èŠ‚
            subsection_match = re.match(r'^####\s*(\d+\.\d+\.\d+(?:\.\d+)*)\s*(.+)$', line)
            if subsection_match and current_chapter:
                subsection_num = subsection_match.group(1)
                subsection_title = subsection_match.group(2).strip()
                structure["toc"].append({
                    "level": 4,
                    "type": "subsection",
                    "chapter": current_chapter,
                    "number": subsection_num,
                    "title": subsection_title
                })

        # æå–å®šä¹‰ï¼ˆç¼©å†™å’Œç¬¦å·éƒ¨åˆ†ï¼‰
        def_pattern = r'([A-Za-z][A-Za-z0-9\s\-/]+?)\s*[â€”â€“\-]\s*(.+?)(?=\n|$)'
        def_sections = re.finditer(r'##\s*.*?(?:ç¼©å†™|ç¬¦å·|å®šä¹‰).*?\n(.*?)(?=\n##|$)',
                                   self.content, re.DOTALL | re.IGNORECASE)

        for match in def_sections:
            def_text = match.group(1)
            definitions = re.findall(def_pattern, def_text)
            for key, value in definitions:
                key_clean = key.strip()
                value_clean = value.strip()
                if len(key_clean) > 1 and len(value_clean) > 3:
                    structure["definitions"][key_clean] = value_clean

        # æå–è¡¨æ ¼å’Œå›¾ç‰‡å¼•ç”¨
        table_pattern = r'è¡¨\s*(\d+\.\d+(?:\.\d+)*)[\.\s]*([^ã€‚]+)'
        figure_pattern = r'å›¾\s*(\d+\.\d+(?:\.\d+)*)[\.\s]*([^ã€‚]+)'

        for match in re.finditer(table_pattern, self.content):
            table_num = match.group(1)
            table_desc = match.group(2).strip()
            structure["tables"][f"è¡¨{table_num}"] = table_desc

        for match in re.finditer(figure_pattern, self.content):
            fig_num = match.group(1)
            fig_desc = match.group(2).strip()
            structure["figures"][f"å›¾{fig_num}"] = fig_desc

        return structure

    def _chunk_content(self, chunk_size: int = 2000) -> List[Dict]:
        """å°†å†…å®¹åˆ†å—ï¼Œä¾¿äºæ£€ç´¢"""
        chunks = []
        lines = self.content.split('\n')

        current_chunk = []
        current_chapter = ""
        current_section = ""

        for line in lines:
            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
            chapter_match = re.match(r'^##\s*ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)\s*ç« \s*(.+)$', line)
            if chapter_match:
                if current_chunk:
                    chunks.append({
                        "content": '\n'.join(current_chunk),
                        "chapter": current_chapter,
                        "section": current_section,
                        "keywords": self._extract_keywords('\n'.join(current_chunk))
                    })
                    current_chunk = []

                chapter_num = chapter_match.group(1)
                chapter_title = chapter_match.group(2).strip()
                current_chapter = f"ç¬¬{chapter_num}ç«  {chapter_title}"
                current_section = ""
                current_chunk.append(line)
                continue

            # æ£€æµ‹å°èŠ‚æ ‡é¢˜
            section_match = re.match(r'^###\s*(\d+\.\d+(?:\.\d+)*)\s*(.+)$', line)
            if section_match:
                if current_chunk and len('\n'.join(current_chunk)) > 100:
                    chunks.append({
                        "content": '\n'.join(current_chunk),
                        "chapter": current_chapter,
                        "section": current_section,
                        "keywords": self._extract_keywords('\n'.join(current_chunk))
                    })
                    current_chunk = []

                section_num = section_match.group(1)
                section_title = section_match.group(2).strip()
                current_section = f"{section_num} {section_title}"
                current_chunk.append(line)
                continue

            current_chunk.append(line)

            # å¦‚æœå½“å‰å—å¤ªå¤§ï¼Œåˆ†å‰²
            if len('\n'.join(current_chunk)) > chunk_size:
                chunks.append({
                    "content": '\n'.join(current_chunk),
                    "chapter": current_chapter,
                    "section": current_section,
                    "keywords": self._extract_keywords('\n'.join(current_chunk))
                })
                current_chunk = []

        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append({
                "content": '\n'.join(current_chunk),
                "chapter": current_chapter,
                "section": current_section,
                "keywords": self._extract_keywords('\n'.join(current_chunk))
            })

        return chunks

    def _build_keyword_index(self) -> Dict[str, List[int]]:
        """æ„å»ºå…³é”®è¯ç´¢å¼•"""
        index = defaultdict(list)

        for i, chunk in enumerate(self.chunked_content):
            if "keywords" in chunk:
                for keyword in chunk["keywords"]:
                    index[keyword.lower()].append(i)

        # æ·»åŠ ç¼©å†™åˆ°ç´¢å¼•
        for abbr in self.structure["definitions"].keys():
            index[abbr.lower()] = list(range(len(self.chunked_content)))  # ç¼©å†™åœ¨æ‰€æœ‰å†…å®¹ä¸­æœç´¢

        return index

    def _extract_keywords(self, text: str, max_keywords: int = 20) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        keywords = set()

        # æå–å¤§å†™ç¼©å†™
        abbreviations = re.findall(r'\b[A-Z]{2,}[A-Z0-9/]*\b', text)
        keywords.update(abbreviations)

        # æå–ä¸­æ–‡ä¸“ä¸šæœ¯è¯­
        chinese_terms = re.findall(r'[\u4e00-\u9fa5]{2,8}', text)

        # è¿‡æ»¤å¸¸è§è¯
        stop_words = {'å¯ä»¥', 'ä¸€ä¸ª', 'è¿›è¡Œ', 'éœ€è¦', 'è¦æ±‚', 'å¦‚æœ', 'åº”å½“', 'å¿…é¡»', 'ä¸å¾—'}
        for term in chinese_terms:
            if term not in stop_words and len(term) >= 2:
                # åªä¿ç•™å‡ºç°é¢‘ç‡è¾ƒé«˜çš„æœ¯è¯­
                if text.count(term) >= 2:
                    keywords.add(term)

        # æå–æ•°å­—ç›¸å…³æœ¯è¯­
        number_refs = re.findall(r'(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |ç¬¬\d+\.\d+æ¡|è¡¨\d+\.\d+|å›¾\d+\.\d+)', text)
        keywords.update(number_refs)

        # æœºåœºç‰¹å®šæœ¯è¯­
        airport_terms = ['è·‘é“', 'æ»‘è¡Œé“', 'æœºåª', 'èˆªç«™æ¥¼', 'ç¯å…‰', 'æ ‡å¿—', 'æ ‡è®°', 'é“é¢',
                         'å‡€ç©º', 'éšœç¢ç‰©', 'ILS', 'VOR', 'NDB', 'PCN', 'ACN', 'RESA',
                         'è·‘é“ç«¯å®‰å…¨åŒº', 'å‡é™å¸¦', 'ç²¾å¯†è¿›è¿‘', 'éç²¾å¯†è¿›è¿‘']

        for term in airport_terms:
            if term in text:
                keywords.add(term)

        return list(keywords)[:max_keywords]

    def get_table_of_contents(self, detailed: bool = True) -> str:
        """è·å–ç›®å½•"""
        toc_lines = ["=" * 80]
        toc_lines.append("é™„ä»¶14ç¬¬Iå·ï¼ˆæœºåœºè®¾è®¡ä¸è¿è¡Œï¼‰ç›®å½•")
        toc_lines.append("=" * 80)

        for item in self.structure["toc"]:
            indent = "  " * (item["level"] - 2)

            if item["type"] == "chapter":
                toc_lines.append(f"{indent}{item['number']} {item['title']}")
            elif item["type"] == "section":
                toc_lines.append(f"{indent}  {item['number']} {item['title']}")
            elif item["type"] == "subsection" and detailed:
                toc_lines.append(f"{indent}    {item['number']} {item['title']}")

        # æ·»åŠ å®šä¹‰éƒ¨åˆ†
        if self.structure["definitions"]:
            toc_lines.append("\nç¼©å†™å’Œç¬¦å·è¡¨:")
            definitions_list = list(self.structure["definitions"].items())[:15]
            for abbr, meaning in definitions_list:
                toc_lines.append(f"  {abbr} â€” {meaning}")
            if len(self.structure["definitions"]) > 15:
                toc_lines.append(f"  ... è¿˜æœ‰{len(self.structure["definitions"]) - 15}ä¸ªå®šä¹‰")

        # æ·»åŠ å¸¸ç”¨æœç´¢å»ºè®®
        toc_lines.append("\n" + "-" * 80)
        toc_lines.append("å¸¸ç”¨æœç´¢å…³é”®è¯:")
        common_keywords = [
            "è·‘é“å®½åº¦", "è·‘é“é•¿åº¦", "æ»‘è¡Œé“", "æœºåª", "æ ‡å¿—", "æ ‡è®°", "ç¯å…‰",
            "PCN", "ACN", "ILS", "éšœç¢ç‰©", "å‡€ç©º", "è·‘é“ç«¯å®‰å…¨åŒº", "å‡é™å¸¦"
        ]
        toc_lines.append("  " + " | ".join(common_keywords))

        return '\n'.join(toc_lines)

    def semantic_search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """è¯­ä¹‰æœç´¢ç›¸å…³æ®µè½"""
        query_keywords = self._extract_keywords(query, max_keywords=10)

        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = []
        for i, chunk in enumerate(self.chunked_content):
            score = 0

            # å…³é”®è¯åŒ¹é…
            chunk_text = chunk["content"].lower()
            for keyword in query_keywords:
                keyword_lower = keyword.lower()
                if keyword_lower in chunk_text:
                    # è®¡ç®—TF
                    tf = chunk_text.count(keyword_lower) / len(chunk_text.split())
                    score += tf * 10

                    # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
                    if chunk.get("section") and keyword_lower in chunk["section"].lower():
                        score += 5
                    if chunk.get("chapter") and keyword_lower in chunk["chapter"].lower():
                        score += 3

            # è€ƒè™‘ç« èŠ‚çš„é‡è¦æ€§
            if chunk.get("chapter") and any(term in chunk["chapter"] for term in ["å®šä¹‰", "æœ¯è¯­", "æ€»åˆ™"]):
                score *= 0.8  # é™ä½å®šä¹‰ç« èŠ‚çš„æƒé‡

            if score > 0:
                scores.append((i, score))

        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x[1], reverse=True)

        # æ„å»ºç»“æœ
        results = []
        for idx, score in scores[:top_k]:
            chunk = self.chunked_content[idx]
            # æå–æŸ¥è¯¢ç›¸å…³ä¸Šä¸‹æ–‡
            context = self._extract_relevant_context(chunk["content"], query)

            results.append(SearchResult(
                content=context,
                chapter=chunk.get("chapter", ""),
                section=chunk.get("section", ""),
                confidence=min(score / 100, 1.0),
                keywords=self._extract_keywords(context, max_keywords=5)
            ))

        return results

    def _extract_relevant_context(self, text: str, query: str, context_chars: int = 800) -> str:
        """æå–æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ"""
        # æ‰¾åˆ°å…³é”®è¯æœ€å¯†é›†çš„åŒºåŸŸ
        query_keywords = self._extract_keywords(query, max_keywords=10)

        lines = text.split('\n')
        best_start = 0
        best_score = 0

        for i in range(len(lines)):
            window_lines = lines[i:i + 10]
            window_text = '\n'.join(window_lines)

            score = 0
            for keyword in query_keywords:
                if keyword.lower() in window_text.lower():
                    score += 1
                    # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
                    if any(line.strip().startswith('#') for line in window_lines):
                        score += 2

            if score > best_score:
                best_score = score
                best_start = i

        # æå–ä¸Šä¸‹æ–‡
        start = max(0, best_start - 5)
        end = min(len(lines), best_start + 15)
        context_lines = lines[start:end]

        return '\n'.join(context_lines)

    def generate_search_suggestions(self, question: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å»ºè®®"""
        keywords = self._extract_keywords(question, max_keywords=10)
        suggestions = []

        # åŸºäºå…³é”®è¯çš„ç« èŠ‚å»ºè®®
        keyword_to_chapter = {
            'è·‘é“': ['ç¬¬3ç«  ç‰©ç†ç‰¹æ€§', 'ç¬¬5ç«  ç›®è§†åŠ©èˆªè®¾æ–½'],
            'æ»‘è¡Œé“': ['ç¬¬3ç«  ç‰©ç†ç‰¹æ€§', 'ç¬¬5ç«  ç›®è§†åŠ©èˆªè®¾æ–½'],
            'ç¯å…‰': ['ç¬¬5ç«  ç›®è§†åŠ©èˆªè®¾æ–½'],
            'æ ‡å¿—': ['ç¬¬5ç«  ç›®è§†åŠ©èˆªè®¾æ–½'],
            'éšœç¢ç‰©': ['ç¬¬4ç«  éšœç¢ç‰©çš„é™åˆ¶å’Œç§»é™¤'],
            'å‡€ç©º': ['ç¬¬4ç«  éšœç¢ç‰©çš„é™åˆ¶å’Œç§»é™¤'],
            'PCN': ['ç¬¬2ç«  æœºåœºæ•°æ®', 'é™„å½•1'],
            'ACN': ['ç¬¬2ç«  æœºåœºæ•°æ®', 'é™„å½•1'],
            'ILS': ['ç¬¬5ç«  ç›®è§†åŠ©èˆªè®¾æ–½', 'é™„å½•'],
            'ç²¾å¯†è¿›è¿‘': ['ç¬¬5ç«  ç›®è§†åŠ©èˆªè®¾æ–½', 'é™„å½•'],
        }

        for keyword in keywords:
            if keyword in keyword_to_chapter:
                suggestions.extend(keyword_to_chapter[keyword])

        # å»é‡
        suggestions = list(dict.fromkeys(suggestions))

        # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œç»™å‡ºä¸€èˆ¬æ€§å»ºè®®
        if not suggestions:
            suggestions = [
                "æŸ¥çœ‹ç¬¬3ç«  'ç‰©ç†ç‰¹æ€§' è·å–è·‘é“ã€æ»‘è¡Œé“å°ºå¯¸è¦æ±‚",
                "æŸ¥çœ‹ç¬¬5ç«  'ç›®è§†åŠ©èˆªè®¾æ–½' è·å–ç¯å…‰ã€æ ‡å¿—è§„èŒƒ",
                "æŸ¥çœ‹ç¬¬4ç«  'éšœç¢ç‰©çš„é™åˆ¶å’Œç§»é™¤' è·å–å‡€ç©ºè¦æ±‚",
                "ä½¿ç”¨ç¼©å†™å¦‚ 'PCN', 'ACN', 'RESA' è¿›è¡Œç²¾ç¡®æœç´¢"
            ]

        return suggestions[:5]

    def ask_question(self, question: str, use_ai: bool = True) -> Dict:
        """
        å›ç­”é—®é¢˜ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_ai: æ˜¯å¦ä½¿ç”¨AIç”Ÿæˆç­”æ¡ˆ

        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå‚è€ƒä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()

        print(f"\nğŸ” æ­£åœ¨æœç´¢: '{question}'")

        # 1. è¯­ä¹‰æœç´¢
        search_results = self.semantic_search(question, top_k=5)
        search_time = time.time() - start_time
        print(f"âœ“ æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ®µè½ï¼Œè€—æ—¶: {search_time:.2f}ç§’")

        # 2. ç”Ÿæˆæœç´¢å»ºè®®
        suggestions = self.generate_search_suggestions(question)

        # 3. å‡†å¤‡ä¸Šä¸‹æ–‡
        context = self._prepare_context(question, search_results)

        # 4. ç”Ÿæˆç­”æ¡ˆ
        if use_ai and search_results:
            print("ğŸ¤– æ­£åœ¨ç”ŸæˆAIç­”æ¡ˆ...")
            answer, confidence = self._generate_ai_answer_with_context(question, context, search_results)
        else:
            print("ğŸ“ ç”ŸæˆåŸºäºæ£€ç´¢çš„ç­”æ¡ˆ...")
            answer = self._generate_retrieval_answer(question, search_results)
            confidence = 0.7 if search_results else 0.3

        # 5. æ„å»ºå“åº”
        response = {
            "question": question,
            "answer": answer,
            "confidence": confidence,
            "references": [],
            "search_suggestions": suggestions,
            "related_keywords": self._extract_keywords(question, max_keywords=8),
            "search_time": search_time,
            "sources": []
        }

        # 6. æ·»åŠ å¼•ç”¨æ¥æº
        for result in search_results:
            if result.confidence > 0.3:  # åªæ·»åŠ ç½®ä¿¡åº¦è¾ƒé«˜çš„æ¥æº
                response["references"].append({
                    "content": result.content[:300] + "..." if len(result.content) > 300 else result.content,
                    "chapter": result.chapter,
                    "section": result.section,
                    "confidence": result.confidence,
                    "keywords": result.keywords
                })

                # æ·»åŠ åˆ°æºåˆ—è¡¨
                source_id = f"{result.chapter}_{hash(result.content) % 10000:04d}"
                response["sources"].append({
                    "id": source_id,
                    "ref": f"æ¥è‡ª{result.chapter}ï¼Œ{result.section}",
                    "excerpt": result.content[:150] + "..."
                })

        # 7. è®°å½•å¯¹è¯å†å²
        conversation_turn = ConversationTurn(
            question=question,
            answer=answer,
            references=response["references"],
            timestamp=time.time()
        )
        self.conversation_history.append(conversation_turn)

        # é™åˆ¶å†å²é•¿åº¦
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]

        return response

    def _prepare_context(self, question: str, search_results: List[SearchResult], max_tokens: int = 30000) -> str:
        """å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []

        # æ·»åŠ ä¼šè¯å†å²ï¼ˆæœ€è¿‘3è½®ï¼‰
        if len(self.conversation_history) > 1:
            context_parts.append("ä¹‹å‰çš„å¯¹è¯:")
            for i, turn in enumerate(self.conversation_history[-3:-1]):
                context_parts.append(f"ç”¨æˆ·: {turn.question}")
                context_parts.append(f"ç³»ç»Ÿ: {turn.answer[:200]}...")
            context_parts.append("")

        # æ·»åŠ ç›¸å…³æœç´¢ç»“æœ
        context_parts.append("ç›¸å…³æ‰‹å†Œå†…å®¹:")

        used_content = set()
        total_length = 0

        for result in search_results:
            if result.confidence > 0.2:  # åªæ·»åŠ ç½®ä¿¡åº¦è¾ƒé«˜çš„ç»“æœ
                content_hash = hash(result.content[:500])
                if content_hash not in used_content:
                    content_with_ref = f"[æ¥æº: {result.chapter}, {result.section}]\n{result.content}\n"

                    if total_length + len(content_with_ref) < max_tokens:
                        context_parts.append(content_with_ref)
                        total_length += len(content_with_ref)
                        used_content.add(content_hash)

        # æ·»åŠ ç›¸å…³å®šä¹‰
        context_parts.append("\nç›¸å…³å®šä¹‰:")
        question_keywords = self._extract_keywords(question)
        for keyword in question_keywords[:5]:
            definition = self.get_definition(keyword)
            if definition:
                context_parts.append(f"{keyword}: {definition}")

        return '\n'.join(context_parts)

    def _generate_ai_answer_with_context(self, question: str, context: str, search_results: List[SearchResult]) -> \
    Tuple[str, float]:
        """ä½¿ç”¨AIç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
        try:
            # æ„å»ºprompt
            prompt = f"""ä½ æ˜¯ä¸€åå›½é™…æ°‘èˆªç»„ç»‡é™„ä»¶14ï¼ˆæœºåœºè®¾è®¡ä¸è¿è¡Œï¼‰çš„ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹æ‰‹å†Œå†…å®¹å›ç­”é—®é¢˜ã€‚

{context}

å½“å‰é—®é¢˜ï¼š{question}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ï¼Œç›´æ¥é’ˆå¯¹é—®é¢˜
2. å¼•ç”¨å…·ä½“æ¥æºï¼ˆç« èŠ‚å·ã€å°èŠ‚å·ï¼‰
3. å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼ŒåŸºäºç›¸å…³çŸ¥è¯†ç»™å‡ºå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶è¯´æ˜ä¸ç¡®å®šæ€§
4. å›ç­”è¦å…·ä½“ï¼Œé¿å…æ¨¡ç³Šè¡¨è¿°
5. å¯¹äºæ“ä½œæ€§é—®é¢˜ï¼Œç»™å‡ºå…·ä½“æ­¥éª¤æˆ–æ ‡å‡†
6. å¦‚æœç›¸å…³ï¼ŒæåŠç›¸å…³è¡¨æ ¼æˆ–å›¾è¡¨

ä¸“ä¸šå›ç­”ï¼š"""

            # è°ƒç”¨API
            response = client.chat.completions.create(
                model="Qwen/Qwen2.5-72B-Instruct",
                messages=[
                    {"role": "system",
                     "content": "ä½ æ˜¯å›½é™…æ°‘èˆªç»„ç»‡é™„ä»¶14ï¼ˆæœºåœºè®¾è®¡ä¸è¿è¡Œï¼‰ä¸“å®¶ï¼Œä¸“é—¨ä¸ºæœºåœºå·¥ä½œäººå‘˜æä¾›ä¸“ä¸šæŒ‡å¯¼ã€‚ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šä½†æ˜“æ‡‚ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000,
                top_p=0.9
            )

            answer = response.choices[0].message.content

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæœç´¢ç»“æœçš„å¹³å‡ç½®ä¿¡åº¦ï¼‰
            if search_results:
                avg_confidence = sum(r.confidence for r in search_results) / len(search_results)
            else:
                avg_confidence = 0.5

            # å¦‚æœå›ç­”ä¸­åŒ…å«"ä¸ç¡®å®š"ã€"ä¸çŸ¥é“"ç­‰è¯ï¼Œé™ä½ç½®ä¿¡åº¦
            uncertainty_indicators = ['ä¸ç¡®å®š', 'æ— æ³•ç¡®å®š', 'ä¸çŸ¥é“', 'æ²¡æœ‰æ‰¾åˆ°', 'æœªæåŠ', 'è¯·æŸ¥é˜…']
            if any(indicator in answer for indicator in uncertainty_indicators):
                avg_confidence *= 0.7

            return answer, min(avg_confidence, 0.95)

        except Exception as e:
            print(f"âš ï¸ AIç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°æ£€ç´¢ç­”æ¡ˆ
            backup_answer = self._generate_retrieval_answer(question, search_results)
            return backup_answer, 0.5

    def _generate_retrieval_answer(self, question: str, search_results: List[SearchResult]) -> str:
        """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ"""
        if not search_results:
            return "åœ¨æ‰‹å†Œä¸­æ²¡æœ‰æ‰¾åˆ°ç›´æ¥ç›¸å…³çš„å†…å®¹ã€‚å»ºè®®ï¼š\n1. æŸ¥çœ‹ç¬¬3ç«  'ç‰©ç†ç‰¹æ€§' å’Œç¬¬5ç«  'ç›®è§†åŠ©èˆªè®¾æ–½'\n2. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„æœ¯è¯­ï¼Œå¦‚ 'è·‘é“å®½åº¦' è€Œé 'è·‘é“'\n3. æŸ¥çœ‹ç¼©å†™è¡¨è·å–æœ¯è¯­å®šä¹‰"

        # æ•´ç†ç­”æ¡ˆ
        answer_parts = ["åŸºäºé™„ä»¶14æ‰‹å†Œï¼Œç›¸å…³ä¿¡æ¯å¦‚ä¸‹ï¼š\n"]

        # æ·»åŠ å®šä¹‰
        question_keywords = self._extract_keywords(question)
        definitions_found = []

        for keyword in question_keywords[:3]:
            definition = self.get_definition(keyword)
            if definition:
                definitions_found.append(f"â€¢ {keyword}: {definition}")

        if definitions_found:
            answer_parts.append("ç›¸å…³å®šä¹‰ï¼š")
            answer_parts.extend(definitions_found)
            answer_parts.append("")

        # æ·»åŠ ä¸»è¦å†…å®¹
        answer_parts.append("æ‰‹å†Œç›¸å…³å†…å®¹ï¼š")

        for i, result in enumerate(search_results[:3], 1):
            source_info = f"[æ¥æº: {result.chapter}"
            if result.section:
                source_info += f", {result.section}"
            source_info += "]"

            summary = result.content[:200].replace('\n', ' ')
            if len(result.content) > 200:
                summary += "..."

            answer_parts.append(f"{i}. {source_info} {summary}")

        # æ·»åŠ å»ºè®®
        answer_parts.append("\nè¿›ä¸€æ­¥å»ºè®®ï¼š")
        answer_parts.append("â€¢ æŸ¥çœ‹å…·ä½“ç« èŠ‚è·å–è¯¦ç»†ä¿¡æ¯")
        answer_parts.append("â€¢ æ³¨æ„æ ‡å‡†çš„é€‚ç”¨èŒƒå›´å’Œæ¡ä»¶")
        answer_parts.append("â€¢ å®é™…åº”ç”¨æ—¶è¯·å‚è€ƒæœ€æ–°ç‰ˆæœ¬å’Œå½“åœ°è§„ç« ")

        return '\n'.join(answer_parts)

    def get_definition(self, term: str) -> Optional[str]:
        """è·å–æœ¯è¯­å®šä¹‰"""
        term_clean = term.strip()

        # ç›´æ¥æŸ¥æ‰¾
        if term_clean in self.structure["definitions"]:
            return self.structure["definitions"][term_clean]

        # å°è¯•æŸ¥æ‰¾è¿‘ä¼¼
        for key, value in self.structure["definitions"].items():
            if term_clean.upper() == key.upper() or term_clean in value:
                return f"{key}: {value}"

        # å°è¯•éƒ¨åˆ†åŒ¹é…
        for key, value in self.structure["definitions"].items():
            if term_clean.upper() in key.upper() or key.upper() in term_clean.upper():
                return f"{key}: {value}"

        return None

    def show_conversation_history(self, max_turns: int = 5) -> str:
        """æ˜¾ç¤ºå¯¹è¯å†å²"""
        if not self.conversation_history:
            return "æš‚æ— å¯¹è¯å†å²ã€‚"

        history_lines = ["=" * 60]
        history_lines.append("å¯¹è¯å†å²")
        history_lines.append("=" * 60)

        start_idx = max(0, len(self.conversation_history) - max_turns)

        for i, turn in enumerate(self.conversation_history[start_idx:], start_idx + 1):
            history_lines.append(f"\n[{i}] Q: {turn.question}")
            history_lines.append(f"   A: {turn.answer[:150]}...")
            history_lines.append(f"   æ—¶é—´: {time.strftime('%H:%M:%S', time.localtime(turn.timestamp))}")

        return '\n'.join(history_lines)

    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            "session_id": self.session_id,
            "manual_loaded": len(self.content) > 0,
            "content_length": len(self.content),
            "chapters_count": len(self.structure["chapters"]),
            "definitions_count": len(self.structure["definitions"]),
            "conversation_turns": len(self.conversation_history),
            "keyword_index_size": len(self.keyword_index),
            "chunks_count": len(self.chunked_content),
            "max_context_length": self.max_context_length
        }


def display_answer(response: Dict):
    """ç¾è§‚åœ°æ˜¾ç¤ºç­”æ¡ˆ"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ é—®é¢˜:", response["question"])
    print("=" * 80)

    # æ˜¾ç¤ºç½®ä¿¡åº¦
    confidence_emoji = "ğŸ”´"
    if response["confidence"] > 0.8:
        confidence_emoji = "ğŸŸ¢"
    elif response["confidence"] > 0.6:
        confidence_emoji = "ğŸŸ¡"

    print(f"{confidence_emoji} ç½®ä¿¡åº¦: {response['confidence']:.1%}")
    print(f"â±ï¸ æœç´¢è€—æ—¶: {response['search_time']:.2f}ç§’")

    print("\n" + "-" * 80)
    print("ğŸ’¡ ç­”æ¡ˆ:")
    print("-" * 80)
    print(response["answer"])

    # æ˜¾ç¤ºæ¥æº
    if response.get("references"):
        print("\n" + "-" * 80)
        print("ğŸ“š å‚è€ƒæ¥æº:")
        print("-" * 80)
        for i, ref in enumerate(response["references"][:3], 1):
            print(f"\n{i}. {ref['chapter']}")
            if ref.get('section'):
                print(f"   å°èŠ‚: {ref['section']}")
            print(f"   ç›¸å…³åº¦: {ref['confidence']:.1%}")
            print(f"   å†…å®¹æ‘˜è¦: {ref['content'][:200]}...")

    # æ˜¾ç¤ºå…³é”®è¯
    if response.get("related_keywords"):
        print("\n" + "-" * 80)
        print("ğŸ”‘ ç›¸å…³å…³é”®è¯:")
        print("-" * 80)
        print(" | ".join(response["related_keywords"][:10]))

    # æ˜¾ç¤ºå»ºè®®
    if response.get("search_suggestions"):
        print("\n" + "-" * 80)
        print("ğŸ’¡ æœç´¢å»ºè®®:")
        print("-" * 80)
        for i, suggestion in enumerate(response["search_suggestions"], 1):
            print(f"{i}. {suggestion}")

    print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ–‡ä»¶è·¯å¾„
    manual_path = r"D:\AlgorithmClub\Damoxingyuanli\homework\datas\é™„ä»¶14 æœºåœº  â€” æœºåœºè®¾è®¡ä¸è¿è¡Œ_ç¬¬Iå· (ç¬¬ä¹ç‰ˆï¼Œ2022å¹´7æœˆ)\index.md"

    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–é™„ä»¶14æ‰‹å†Œé—®ç­”ç³»ç»Ÿ...")
    print(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„: {manual_path}")

    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        qa_system = EnhancedAttachment14ManualQA(manual_path)

        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        status = qa_system.get_system_status()
        print(f"\nâœ… ç³»ç»ŸçŠ¶æ€:")
        print(f"   â€¢ ä¼šè¯ID: {status['session_id']}")
        print(f"   â€¢ æ‰‹å†ŒåŠ è½½: {'æˆåŠŸ' if status['manual_loaded'] else 'å¤±è´¥'}")
        print(f"   â€¢ ç« èŠ‚æ•°: {status['chapters_count']}")
        print(f"   â€¢ å®šä¹‰æ•°: {status['definitions_count']}")
        print(f"   â€¢ å†…å®¹å—: {status['chunks_count']}")

        # æ˜¾ç¤ºç›®å½•
        print("\n" + "=" * 80)
        toc = qa_system.get_table_of_contents(detailed=True)
        print(toc)

        # ç¤ºä¾‹é—®é¢˜
        print("\n" + "=" * 80)
        print("ğŸ’¡ ç¤ºä¾‹é—®é¢˜ï¼ˆæ‚¨å¯ä»¥ç›´æ¥è¾“å…¥æ•°å­—é€‰æ‹©ï¼‰:")
        print("=" * 80)
        example_questions = [
            "1. è·‘é“ç«¯å®‰å…¨åŒº(RESA)çš„å°ºå¯¸è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
            "2. PCNå’ŒACNåˆ†åˆ«ä»£è¡¨ä»€ä¹ˆï¼Ÿå¦‚ä½•è®¡ç®—ï¼Ÿ",
            "3. è·‘é“å®½åº¦å’Œé•¿åº¦çš„åŸºæœ¬è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
            "4. ç›®è§†è¿›è¿‘å¡åº¦æŒ‡ç¤ºç³»ç»Ÿ(VASIS)çš„å¸ƒç½®è¦æ±‚ï¼Ÿ",
            "5. éšœç¢ç‰©é™åˆ¶é¢åŒ…æ‹¬å“ªäº›ï¼Ÿå„è‡ªçš„æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "6. æ»‘è¡Œé“çš„æœ€å°å®½åº¦è¦æ±‚æ˜¯å¤šå°‘ï¼Ÿ",
            "7. è·‘é“æ ‡å¿—å’Œæ»‘è¡Œé“æ ‡å¿—æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "8. æœºåœºç¯å…‰ç³»ç»Ÿæœ‰å“ªäº›ç±»å‹ï¼Ÿ",
            "9. ç²¾å¯†è¿›è¿‘è·‘é“å’Œéç²¾å¯†è¿›è¿‘è·‘é“çš„åŒºåˆ«ï¼Ÿ",
            "10. æœºåœºé“é¢å¼ºåº¦æŠ¥å‘ŠPCNå¦‚ä½•è§£è¯»ï¼Ÿ"
        ]

        for q in example_questions:
            print(q)

        print("\n" + "=" * 80)
        print("ğŸ’¬ å¼€å§‹å¯¹è¯ (è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©, 'quit' é€€å‡º)")
        print("=" * 80)

        # äº¤äº’å¼é—®ç­”
        while True:
            try:
                print("\n" + "-" * 80)
                user_input = input("\nğŸ’­ è¯·è¾“å…¥é—®é¢˜æˆ–å‘½ä»¤: ").strip()

                if not user_input:
                    continue

                # å‘½ä»¤å¤„ç†
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                elif user_input.lower() in ['help', 'å¸®åŠ©', '?']:
                    print("\nğŸ“‹ å¯ç”¨å‘½ä»¤:")
                    print("  help      - æ˜¾ç¤ºå¸®åŠ©")
                    print("  toc       - æ˜¾ç¤ºç›®å½•")
                    print("  history   - æ˜¾ç¤ºå¯¹è¯å†å²")
                    print("  status    - æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€")
                    print("  keywords  - æ˜¾ç¤ºå¸¸ç”¨å…³é”®è¯")
                    print("  clear     - æ¸…é™¤å¯¹è¯å†å²")
                    print("  quit      - é€€å‡ºç³»ç»Ÿ")
                    print("\nğŸ’¡ æç¤ºï¼š")
                    print("  â€¢ è¾“å…¥æ•°å­—1-10é€‰æ‹©ç¤ºä¾‹é—®é¢˜")
                    print("  â€¢ ä½¿ç”¨å…·ä½“æœ¯è¯­æé—®æ›´ç²¾ç¡®")
                    print("  â€¢ å¯ä»¥è¿ç»­æé—®ï¼Œç³»ç»Ÿä¼šè®°ä½ä¸Šä¸‹æ–‡")
                    continue

                elif user_input.lower() == 'toc':
                    print(qa_system.get_table_of_contents(detailed=True))
                    continue

                elif user_input.lower() == 'history':
                    print(qa_system.show_conversation_history())
                    continue

                elif user_input.lower() == 'status':
                    status = qa_system.get_system_status()
                    print(f"\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
                    for key, value in status.items():
                        print(f"  {key}: {value}")
                    continue

                elif user_input.lower() == 'keywords':
                    print("\nğŸ”‘ å¸¸ç”¨å…³é”®è¯:")
                    keywords = [
                        "è·‘é“", "æ»‘è¡Œé“", "æœºåª", "ç¯å…‰", "æ ‡å¿—", "PCN", "ACN",
                        "ILS", "VOR", "NDB", "RESA", "éšœç¢ç‰©", "å‡€ç©º", "é“é¢",
                        "å‡é™å¸¦", "è·‘é“ç«¯", "ç²¾å¯†è¿›è¿‘", "éç²¾å¯†è¿›è¿‘", "ç›®è§†åŠ©èˆª"
                    ]
                    print(" | ".join(keywords))
                    continue

                elif user_input.lower() == 'clear':
                    qa_system.conversation_history = []
                    print("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…é™¤")
                    continue

                # å¤„ç†æ•°å­—é€‰æ‹©ç¤ºä¾‹é—®é¢˜
                elif user_input.isdigit() and 1 <= int(user_input) <= len(example_questions):
                    idx = int(user_input) - 1
                    actual_question = example_questions[idx].split('. ', 1)[1]
                    print(f"\nğŸ“ é€‰æ‹©é—®é¢˜: {actual_question}")
                    user_input = actual_question

                # å¤„ç†é—®é¢˜
                print(f"\nğŸ§  æ­£åœ¨åˆ†æ: '{user_input}'")

                response = qa_system.ask_question(user_input, use_ai=True)
                display_answer(response)

            except KeyboardInterrupt:
                print("\n\nâš ï¸ ä¸­æ–­æ“ä½œï¼Œè¾“å…¥ 'quit' é€€å‡º")
                continue
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
                print("è¯·é‡æ–°è¾“å…¥æˆ–è¾“å…¥ 'quit' é€€å‡º")
                continue

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{manual_path}'")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥: 1) æ–‡ä»¶è·¯å¾„ 2) APIå¯†é’¥ 3) ç½‘ç»œè¿æ¥")


if __name__ == "__main__":
    main()
